PROMPTS = {
  "ABAP Dictionary": (
        "Analyze SAP Dictionary Display Table screenshot and extract core technical structure .\n\n"
        
        "Output Requirements:\n"
        "{\n"
        "  \"Fields\": [\n"
        "    {\n"
        "      \"FieldName\": \"normalized_name\",\n"
        "      \"DataType\": \"DATA_TYPE\",\n"
        "      \"Length\": number\n"
        "    },\n"
        "    ...\n"
        "  ]\n"
        "}\n\n"
        
        "Field  Processing Rules:\n"
        "1. Name Normalization.\n"
        "   - Convert all to lowercase\n"
        "   - if the 'field' column (or 'Feld' Spalte) contains the followings Substring then rename to:"  
        "   - 'to' → 'date to' (e.g., 'dateto' → 'date to')\n"
        "   - 'from' → 'date from' (e.g., 'datefrom' → 'date from')\n"
        "   - 'txt' → 'length of text' (e.g., 'txtlg', 'txh', 'txtmd'  → 'length of text')\n"
        "   - 'lang' → 'langu'\n\n"
        
        "2. Field Exclusion:\n"
        "   - Remove fields containing '/BIC/'\n"
        "   - Remove personal identifiers\n"
        
        
        "3. Technical Requirements:\n"
        "   - Include only: FieldName, Key, DataType, Length\n"
        "   - Maintain original data types (CHAR, DATS, etc.)\n"
        "   - Keep original length values\n"
        "   - Output must be system-agnostic and reusable\n\n"

    ),
    "BW4Cockpit (Stammdaten)": (
        "Analyze SAP BW4Cockpit table screenshot and extract structural metadata with these exact rules:\n\n"
        
        "1. COLUMN NAME PROCESSING:\n"
        "   - Remove all technical suffixes (e.g., 'TE8 018')\n"
        "   - Apply these name replacements:\n"
        "     * Contains 'bereich' → 'Fachbereich'\n"
        "     * Contains 'to' → 'Date to'\n"
        "     * Contains 'from' → 'Date from'\n"
        "     * Contains 'long' → 'Long text'\n"
        "     * Contains 'short' → 'Short text'\n"
        "     * Contains 'medium' → 'Medium text'\n"
        "     * Just 'date' → 'Date'\n"
        "   - Output cleaned names in 'CleanedColumns' array\n\n"
        
        "2. LONG TEXT COLUMN ANALYSIS:\n"
        "   a) Detection:\n"
        "      - Set 'HasLongTextColumn': true if any column contains 'Long text'\n"
        "      - Otherwise set all long text fields to false/null/empty\n"
        "   b) Validation (if long text exists):\n"
        "      - Set 'LongTextConsistentStructure': true if ANY row contains the following substrings 'BAU - Bauwesen ' not just 'Bauwesen'  and/or. 'GES - Gesundheit ' not just 'Gesundheit' and/or 'MuK - Management' not just 'Management'\n"
        "      - Otherwise set to false and add String 'Exist the non-normalized name'  to 'InvalidLongTextValues'\n\n"
        
        "OUTPUT FORMAT (strict JSON):\n"
        "{\n"
        "  \"CleanedColumns\": [\"Semester\", \"Date from\", ...],\n"
        "  \"HasLongTextColumn\": boolean,\n"
        "  \"LongTextConsistentStructure\": boolean | null,\n"
        "  \"InvalidLongTextValues\": [\"string\"] | []\n"
        "}\n\n"
        
        "VALIDATION RULES:\n"
        "- Never include technical names or student-specific data\n"
        "- Empty array for invalid values when no long text column\n"
        "- If LongTextConsistentStructure is true, there is no  'Long text' column after 'Fachbereich' column\n"
        "- Null only for 'LongTextConsistentStructure' when no long text column\n\n"
        
        "EXAMPLE OUTPUTS:\n"
        "1. With valid long text:\n"
        "{\n"
        "  \"CleanedColumns\": [\"Semester\", \"Faculty\", \"Long text\"],\n"
        "  \"HasLongTextColumn\": true,\n"
        "  \"LongTextConsistentStructure\": true,\n"
        "  \"InvalidLongTextValues\": []\n"
        "}\n\n"
        "2. With invalid long text:\n"
        "{\n"
        "  \"CleanedColumns\": [\"Date to\", \"Fachbereich\", \"Date to\"],\n"
        "  \"HasLongTextColumn\": true,\n"
        "  \"LongTextConsistentStructure\": false,\n"
        "  \"InvalidLongTextValues\": [\"Exist the non-normalized name\"]\n"
        "}\n\n"
        "3. No long text column:\n"
        "{\n"
        "  \"CleanedColumns\": [\"ID\", \"Status\", \"Date\"],\n"
        "  \"HasLongTextColumn\": false,\n"
        "  \"LongTextConsistentStructure\": null,\n"
        "  \"InvalidLongTextValues\": []\n"
        "}"
    ),

    "Data Source": (
        "Analyze a DataSource definition screenshot in SAP BW (Eclipse) and extract these 4 metadata fields:\n\n"
        "1. 'Zu ignorierende Kopfzeilen' (number of header rows to ignore)\n"
        "2. 'Datenseparator' (field separator character presence)\n"
        "3. 'Escape-Zeichen' (field escape character presence)\n"
        "4. 'Zahlenformat festlegen' checkbox status\n\n"
        "Output format (strict JSON only):\n"
        "{\n"
        "  \"IgnoredHeaderRows\": number | null,\n"
        "  \"FieldSeparatorSet\": true | false,\n"
        "  \"EscapeCharacterSet\": true | false,\n"
        "  \"ZahlenformatFestlegen\": true | false\n"
        "}\n\n"
        "Rules:\n"
        "- For header rows: Return null if field is empty\n"
        "- For separators/escape: true if any visible character exists\n"
        "- For checkbox: true only if visibly checked (✓)\n"
        "- Exclude all other information (paths, technical names, etc.)\n"
        "- No additional comments or explanations"
    ),
  "Datenvorschau": (
        "Analyze SAP BW4/HANA data preview screenshot and extract structured data based on table type:\n\n"
        
        "1. For 'Datenvorschau für DataSource' (or 'DataSource Preview') tables (there is no column named 'Key Figures'):\n"
        "   - Extract ONLY column headers\n"
        "   - Return as JSON list under 'Columns' key\n"
        "   - Example: {'Columns': ['Header1', 'Header2', ...]}\n\n"
        
        "2. For 'BW Reporting Vorschau' (or 'BW Reporting Preview') tables (there is a column named 'Key Figures'):\n"
        "   - Ignore column 'Key Figures'\n"
        "   - Extract BOTH column headers AND last row values\n"
        "   - Return as JSON object with headers as keys and last row as values\n"
        "   - Example: {'Header1': 'Value1', 'Header2': 'Value2', ...}\n\n"
        
        "3. Name Normalization.\n"
        "   - Convert all to lowercase. if they contain the following Substrings \n"
        "   'bereich'→'fachbereich', 'bachelor'→'bachelorarbeiten'\n"
        "   'promo' or 'Anzahlpromo'→'promotionen', 'stud' or 'Anzahlstud'→'studierenden'\n"
        "   'dritt' or 'exter'→'drittmittel', 'landes'→'landesmittel'\n"
        "   'semes'→'semester', 'bund'or 'stat'→'bundesland'\n"
        "   'krz'→'kuerzel', 'von' or 'from'→'gueltig von'm 'to' or 'bis' -> 'gueltig bis'\n"
        "   'bez' or 'descri'→'Bezeichnung'\n"
        "   'valid from' = gueltig von, 'valid to'= 'gueltig bis'"
        
        "Processing Rules:\n"
        "• Determine table type by its label before extraction\n"
        ". Names of Column are unique"
        "• For reporting previews, ONLY use the last row (totals row)\n"
        "• Exclude all other interface elements and metadata\n"
        "• Return empty object {} if no recognized table found\n\n"
        

        
        "Output Requirements:\n"
        "- Strict JSON format only\n"
        "- No additional explanations\n"
        "- Case-sensitive headers\n"
        "- Ignore non-data UI elements"
        
        "if the table is 'Datenvorschau für DataSource' (or 'DataSource Preview')  then return OUTPUT  in JSON format (German only) for 'Datenvorschau für DataSource':\n"
        "{\n"
        "   \"Columns\": [\"header1\", \"header2\", ...] | null\n"
        "}"
        "or it is a 'BW Reporting Vorschau' (or 'BW Reporting Preview') table"
        "return OUTPUT  in JSON format (German only) ':\n"
        "{\n"
        "  \"Fields\": [\n"
        "    {\n"
        "      \"FieldName\": \"Header\",\n"
        "      \"Value\": \"number\"\n"
        "    },\n"
        "    ...\n"
        "  ]\n"
        "}\n\n"
    ),

    "Bewegungsdaten": (
    "From the provided image of a structured text data file (Bewegungsdaten in SAP BW), extract the following metadata in JSON format:\n\n"

    "0. TITLE/LABEL ROW SKIPPING:\n"
    "   - Ignore any row that contains label-like text such as 'Bewegungsdaten', 'Teil', or general descriptive titles\n"
    "   - These rows are not headers and should be skipped before identifying actual column names or data rows\n\n"

    "1. DELIMITER DETECTION:\n"
    "   - Identify the primary column separator character\n"
    "   - Common delimiters: ; , | # TAB\n"
    "   - Must appear consistently in first 5 rows (after skipped title/comment rows)\n\n"

    "2. DATA EXTRACTION:\n"
    "   First Data Row:\n"
    "      - Extract first complete non-header row, split it using the delimiter, and output as an array under (FirstRow Array)\n"
    "      - FirstRow Array contains the only values, which are split by the delimiter, not includes the delimiter\n"
    "      - Set \"ColumnCount\" equal to the length of this array\n\n"

    "3. HEADER DETECTION:\n"
    "   - Skip rows that are comments (start with # or //) or contain generic titles (e.g., contain the word 'Bewegungsdaten')\n"
    "   - Header row contains words like 'Bundesland', 'Semester' etc.\n"
    "   - The first row after those lines is considered for header detection\n"
    "   - If this row appears to contain column headers:\n"
    "     * Extract header names\n"
    "     * Output as string array under 'Columnsname'\n"
    "   - Else: Set 'Columnsname': null and treat this as the first data row\n\n"

    "OUTPUT FORMAT (strict JSON):\n"
    "{\n"
    "  \"ColumnDelimiter\": \"character\",\n"
    "  \"FirstRow\": [\"value1\", \"value2\", ...],\n"
    "  \"ColumnCount\": number,\n"
    "  \"Columnsname\": [\"header1\", \"header2\", ...] | null\n"
    "}\n\n"

    "VALIDATION RULES:\n"
    "- Delimiter must be non-alphanumeric\n"
    "- Skip empty/comment rows (starting with # or //)\n"
    "- Skip title/label rows containing substrings like 'Bewegungsdaten'\n"
    "- Trim whitespace from all values\n"
    "- Return null for Columnsname if headers not detectable\n\n"

    "EXAMPLE OUTPUTS:\n"
    "1. With headers:\n"
    "{\n"
    "  \"ColumnDelimiter\": \";\",\n"
    "  \"FirstRow\": [\"A1\", \"B2\", \"C3\", \"D4\"],\n"
    "  \"ColumnCount\": 3,\n"
    "  \"Columnsname\": [\"ID\", \"Date\", \"Status\"]\n"
    "}\n\n"
    "2. Without headers:\n"
    "{\n"
    "  \"ColumnDelimiter\": \",\",\n"
    "  \"FirstRow\": [\"A1\", \"B2\", \"C3\", \"D4\"],\n"
    "  \"ColumnCount\": 4,\n"
    "  \"Columnsname\": null\n"
    "}"
),

    "Transformation": (
        "Analyze SAP BW Transformation screenshot. Extract ONLY TARGET (Ziel) field information following these rules:\n"
        "1. EXTRACT ONLY from Target (Ziel) section - ignore all Source (Quelle) data and connection lines\n"
        "2. REMOVE all technical identifiers (like TE007, TE 007 etc.)\n"
        "3. Ignore fields containing these substrings: 'hochschul', 'zeit', 'ort', 'daten', 'graphie','cordmode'\n"
        "4. MARK fields with key symbol (🔑) in column 'S.' as IsKey=true. dose row 'valid to' contain a key symbol? Check key symbol 3 times. \n"
        "5. NORMALIZE field names if they contain   these following substrings:\n"
        
        
          " calday|calenda : Kalender Tag, \n"
          " bereich|depart : Fachbereich, \n"
          " bache|anzahlbach: Bachelorarbeiten,\n"
          " promo|anzahlpro: Promotionen,\n"
          " stud|anzahlstud:Studierenden,\n"
          " dritt|exter: Drittmittel,\n"
          " landes: Landesmittel,\n"
          " semes: Semester,\n"
          " erende: Semesterende,\n"
          " bund|stat : Bundesland,\n"
          " base: Base Unit of Measure,\n"
          " krz|kür: kuerzel,\n"
          " von|from: gueltig von,\n"
          " to|bis: gueltig bis,\n"
          " bez|descri|extra long: Bezeichnung,\n"
        "6. Return ONLY these Target properties: field name (normalized), type, and key status "
         
        
      
          "OUTPUT FORMAT (strict JSON):\n"
        "{\n"
            "  \"Fields\": [\n"
            "  \"TotalFields\": number\n"
            "  \"TotalKeys\": number\n"
            "    {\n"
                " \"Name \": \"normalized_field_name\",\n"
                "\"Type\": \"field_data_type\",\n"
                " \"IsKey\": true|false\",\n"
        "    ...\n"
        "  ],\n"
        
        "}"

        ),
    "Data Flow Object": (
            "Analyze SAP BW Data Flow Object screenshot in Eclipse. Return:"
            "1. Total count of objects (Objects are blue rectangles on the white background in the image.)."
            "2. Whether ALL objects or the groups of objects (objects, which are linked together)  are interconnected  \n\n"
            "Rules:\n"
            "- Entire system is 'fully connected' if:\n"
            "  There are no isolated objects or groups of objects\n"
            "return Output in JSON format:\n"
            "{\n"
            "  \"TotalObjects\": number,\n"
            "  \"FullyConnected\": true|false\n"
            "}"
        ),
    "DTP": (
        "Analyze DTP (Data Transfer Process) monitor screenshot in SAP BW (Eclipse) and extract:\n"
        "1. All visible data package names\n"
        "2. Their corresponding record counts\n\n"
        
        "Output Format (strict JSON):\n"
        "{\n"
        "  \"DataPackages\": [\n"
        "    {\"Name\": \"package_name\", \"RecordCount\": number},\n"
        "    ...\n"
        "  ]\n"
        "}\n\n"
        
        "Processing Rules:\n"
        "- Extract ONLY from 'Request Processing' section\n"
        "- Include ONLY fully visible rows in the current view\n"
        "- Never infer or summarize hidden/partial data\n"
        "- RecordCount must be numeric (convert from string if needed)\n"
        "- Exclude all other DTP metadata and UI elements\n\n"
        
        "Example Output:\n"
        "{\n"
        "  \"DataPackages\": [\n"
        "    {\"Name\": \"Package_001\", \"RecordCount\": 1250},\n"
        "    {\"Name\": \"Package_002\", \"RecordCount\": 843}\n"
        "  ]\n"
        "}"
    ),

    "Excel": (
        "Analyze Excel screenshot and extract summary data with these exact rules:\n\n"
        
        "1. If BOTH 'Overall Result' COLUMN AND ROW EXIST:\n"
        "   - Extract ONLY intersecting cell values between the column 'Overall Result' and row 'Overall Result'\n"
        "   - Name keys as 'Overall Result_1', 'Overall Result_2',... (left-to-right order)\n"
        "   Example: {\"Overall Result_1\": \"4,500\", \"Overall Result_2\": \"12%\"}\n\n"
        
        "2. If ONLY 'Overall Result' COLUMN EXISTS:\n"
        "   - Use the corresponding ROW name as key name\n"
        "   - Extract the cell value from 'Overall Result' \n"
        "   Example: {\"Region\": \"North\"}\n\n"
        
        "3. If ONLY 'Overall Result' ROW EXISTS:\n"
        "   - Use COLUMN name as key names\n"
        "   - Extract values from 'Overall Result' of the columns\n"
        "   Example: {\"Total Sales\": \"1,234\", \"Profit\": \"567\"}\n\n"
        
        "4. SPECIAL CASES:\n"
        "   - If multiple matches in scenario 2/3, number them sequentially\n"
        "   - Preserve original formatting (treat as strings)\n"
        "   - Return {} if no valid data found\n\n"
        
        "OUTPUT REQUIREMENTS:\n"
        "- Strict JSON format only\n"
        "- No technical metadata\n"
        "- Empty object if no matches\n"
        "- Numbering starts at 1 for multiple values"
    ),
    "Composite Provider": (
        "Analyze SAP BW Composite Provider screenshot. Extract field mappings between Source and Target "
        "following these rules:\n"
        "1. ONLY use visibly connected fields (follow connector lines)\n"
        "2. REMOVE all technical identifiers (like TE007, TE 007 etc.) and then convert them all into lower case\n"
        "3. Ignore fields or rows, which contain  the following substrings:'hochschul', 'zeit', 'ort', 'daten', 'geogra' (e.g. 'geografie')"
        "4. NORMALIZE names of Source and Target if they contain the following Substrings by applying FIRST matching rule:\n"
        "   'bereich' or 'depart'→'Fachbereich', 'bachelor' or 'anzahlbache'→'Bachelorarbeiten'\n"
        "   'promo' or 'anzahlpromo'→'Promotionen', 'studier' or 'anzahlstu'→'Studierenden'\n"
        "   'dritt' or 'exter'→'Drittmittel', 'landes'→'Landesmittel'\n"
        "   'semes'→'Semester', 'bund'or 'stat'→'Bundesland'\n"
        "   'base'→'Base Unit of Measure'\n"
        "5. return OUTPUT  in JSON format (German only):\n"
        "{\n"
        "  \"Mappings\": [\n"
        "  \"TotalMappings\": number\n"
        "    {\"Source\":\"normalized_name_in_source\", \"Target\":\"normalized_name_in_target\"},\n"
        "    ...\n"
        "  ],\n"
        
        "}"
    ),
    "Query": (
        "Analyze SAP BW Query definition screenshot in Eclipse and extract:\n\n"
        
        "1. FIELD EXTRACTION:\n"
        "   - From 'Spalten'/'Columns (Kennzahlen)': Visible field labels (user-facing names only)\n"
        "   - From 'Filter: Festwerter' /'Fixed Values': Visible field labels (user-facing names only)\n"
        "   - From 'Filter: Standardwerter' / 'Default Values': Visible field labels (user-facing names only)\n"
        "   - From 'Zeilen'/'Rows': Visible field labels (user-facing names only)\n"
        "   - From 'Freie Merkmale'/'Free Characteristics': Visible field labels\n"
        "   Processing Rules:\n"
        "   • Remove all technical names (TE-numbers like TE6 017)\n"
        "   • Exclude content in square brackets []\n"
        "   • Keep only plain text labels (e.g., 'Semester', not 'Semester TE8 012')\n\n"
        "2. NORMALIZE names. Convert all into lower case. if they contain the following Substrings by applying FIRST matching rule:\n"
        "   'anzahl bache' →'anzahl bachelorarbeiten'\n"
        "   ' anzahl der prom' →'Anzahl der promotionen'\n"
        "   'anzahl der stud'-> 'anzahl der studierenden'\n"
        "   'fachbe'→'fachbereich'\n"
        "   'land' or 'bundes'→'bundesland'\n"
        "   'seme'→'semester'\n"
        
        
        "3. PROPERTIES EXTRACTION:\n"
        "   - 'Beschreibung'/'Description': Only if no TE-numbers present\n"
        "   - 'Anzahl Dezimalstellen'/'Decimal Places': (e.g. '0,000' -> '3', '0,0'-> '1' , ) \n"
        "     • Value if 'Anzeigen'/'Display' section visible\n"
        "     • 'nicht vorhanden' if section not visible\n\n"
        
    
        "3. QUERY NUMBER EXTRACTION:\n"
        "   - From title (e.g., 'Datenblatt-Definition: TE8_012_Q5_HS')\n"
        "   - Extract only Q1-Q5 format\n"
        "   - For malformed versions (e.g., 'Q112' → 'Q1')\n\n"
        
        "OUTPUT FORMAT (strict JSON):\n"
        "{\n"
        "  \"Query\": \"Q1-Q5\",\n"
        "  \"Columns\": [\"label1\", \"label2\", ...],\n"
        "  \"Rows\": [\"label1\", \"label2\", ...],\n"
        "  \"Filter: Festwerter\": [\"label1\", \"label2\", ...],\n"
        "  \"Filter: Standardwerter\": [\"label1\", \"label2\", ...],\n"
        "  \"FreeCharacteristics\": [\"label1\", \"label2\", ...],\n"
        "  \"Description\": \"text\" | null,\n"
        "  \"DecimalPlaces\": \"number\" | \"nicht vorhanden\"\n"
        "}\n\n"
        
        "VALIDATION RULES:\n"
        "- Return null for Description if it contains TE-numbers\n"
        "- Arrays should be empty if no valid fields found\n"
        "- No technical names in output\n"
        "- No explanations or metadata\n\n"
        
        "EXAMPLE OUTPUT:\n"
        "{\n"
        "  \"Query\": \"Q3\",\n"
        "  \"Columns\": [\"Fachbereich\", \"Anzahl Studierende\"],\n"
        "  \"Rows\": [\"Jahr\", \"Semester\"],\n"
        "  \"Filter: Festwerter\": [\"Semester\"\n"
        "  \"Filter: Standardwerter\": [\"Land\"\n"
        "  \"FreeCharacteristics\": [\"Campus\"],\n"
        "  \"Description\": \"Student enrollment statistics\",\n"
        "  \"DecimalPlaces\": \"2\"\n"
        "}"
    ),
    "Data Mart": (
        "Analyze SAP BW Data Mart screenshot. "
        "Determine ONLY whether the 'DataMart' option is selected in the 'Modellierungseigenschaften' section. "
        "Output format (strict JSON):\n"
        "{\n"
        "  \"DataMartSelected\": true|false\n"
        "}\n\n"
        "Rules:\n"
        "- Return true ONLY if the DataMart radio button or checkbox is visibly selected\n"
        "- Return false if unselected, grayed out, or not present\n"
        "- Output must contain ONLY this boolean value\n"
        "- No additional fields or explanations"
    ),
    "Data Store Object": (
        "Analyze SAP BW Data Store Object screenshot. "
        "Determine ONLY whether the 'Staging-DataStore-Object' option is selected in the 'Modellierungseigenschaften' section. "
        "NORMALIZE names. Convert all into lower case. if they contain the following Substrings: 'staging' -> 'Staging-DataStore-Object' \n"
        "Output format (strict JSON):\n"
        "{\n"
        "  \"Staging-DataStore-Object\": true|false\n"
        "}\n\n"
        "Rules:\n"
        "- Return true ONLY if the 'Staging-DataStore-Object' radio button or checkbox is visibly selected\n"
        "- if the DataMart radio button or checkbox is visibly selected, that means that 'Staging-DataStore-Object' radio button or checkbox is not selected "
        "- Return false if unselected, grayed out, or not present\n"
        "- Output must contain ONLY this boolean value\n"
        "- No additional fields or explanations" ),
}

def get_prompt(photo_type: str) -> str:
  print("Vom Benutzer ausgewählter Bildtyp:", photo_type)
  return PROMPTS.get(photo_type, "Bildbeschreibung und Rückgabe von JSON.")

